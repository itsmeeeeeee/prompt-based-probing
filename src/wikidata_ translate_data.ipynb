{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ER5p4Vtsq9go5-7Ly-35GIc8RXKFGR3v","timestamp":1749832941662}],"collapsed_sections":["38tShNRlWp5T","6LyZWCBeWp8o"],"mount_file_id":"1ER5p4Vtsq9go5-7Ly-35GIc8RXKFGR3v","authorship_tag":"ABX9TyPr5wAZJhTMakyuDFjpbSwQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"eEjGIp5fXsPE","executionInfo":{"status":"ok","timestamp":1750756682833,"user_tz":-120,"elapsed":23595,"user":{"displayName":"Aldi Halili","userId":"04451056983466283749"}},"outputId":"40870164-438a-4579-8ebd-5edb51f1b50f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Wikidata ID (Entitäten ID mit API extrahieren)"],"metadata":{"id":"u28abaZlxLaY"}},{"cell_type":"code","source":["import json, pathlib, time, urllib.parse, requests\n","\n","\n","\n","\n","INPUT_FILE  = \"/content/drive/MyDrive/master_thesis/data/factual_data/zero_shot_factual/new_factual/superhero_person.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/zero_shot_factual/wikidata/superhero_person.json\"\n","LANG        = \"en\"\n","MAX_RETRY   = 3\n","\n","def wikidata_id(label: str, lang: str = LANG) -> str | None:\n","   \"\"\"\n","   Gibt die Wikidata-Q-ID zurück, deren Label exakt mit *label* übereinstimmt.\n","\n","    • Es werden maximal 10 Suchergebnisse abgefragt.\n","    • Erfordert eine exakte Übereinstimmung der Labels (Groß-/Kleinschreibung wird ignoriert,\n","      Leerzeichen am Anfang und Ende werden entfernt).\n","    • Falls keine exakte Übereinstimmung gefunden wird, wird None zurückgegeben, sodass\n","      der Wert als \"N/A\" markieren werden kann.\n","      \"\"\"\n","\n","    base = \"https://www.wikidata.org/w/api.php\"\n","    params = {\n","        \"action\": \"wbsearchentities\",\n","        \"search\": label,\n","        \"language\": lang,\n","        \"format\": \"json\",\n","\n","        \"type\": \"item\",\n","        \"limit\": 10,\n","        \"origin\": \"*\"\n","    }\n","    url = f\"{base}?{urllib.parse.urlencode(params)}\"\n","\n","    for attempt in range(1, MAX_RETRY + 1):\n","        try:\n","            hits = requests.get(url, timeout=30).json().get(\"search\", [])\n","            if hits:\n","                canonical = label.strip().lower()\n","                for h in hits:\n","                    if h.get(\"label\", \"\").strip().lower() == canonical:\n","                        return h[\"id\"]\n","                return None\n","        except Exception as e:\n","            if attempt == MAX_RETRY:\n","                print(f\"  {label}: {e}\")\n","            time.sleep(1.5 * attempt)\n","    return None\n","\n","\n","data = json.loads(pathlib.Path(INPUT_FILE).read_text(encoding=\"utf-8\"))\n","\n","for sample in data[\"samples\"]:\n","    s_label = sample[\"subject\"]\n","    o_label = sample[\"object\"]\n","\n","    sample[\"subject_id\"] = wikidata_id(s_label) or \"N/A\"\n","    sample[\"object_id\"]  = wikidata_id(o_label) or \"N/A\"\n","\n","    time.sleep(5)\n","\n","pathlib.Path(OUTPUT_FILE).write_text(\n","    json.dumps(data, ensure_ascii=False, indent=2), \"utf-8\"\n",")\n","print(f\"Datei mit IDs geschrieben: {OUTPUT_FILE}\")"],"metadata":{"id":"k-Z5ikTfGfUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Shots ID extrahieren"],"metadata":{"id":"sV2gHZvaxZQj"}},{"cell_type":"code","source":["import json, pathlib, time, urllib.parse, requests\n","\n","\n","\n","INPUT_FILE  = \"/content/drive/MyDrive/master_thesis/data/fewshot_examples/factual/few_shots/factual_en.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/fewshot_examples/factual/few_shots/wikidata_id_factual_en.json\"\n","LANG        = \"en\"\n","MAX_RETRY   = 3\n","\n","\n","def wikidata_id_shots(label: str, lang: str = LANG) -> str | None:\n","    \"\"\"\n","    Gibt die Wikidata-Q-ID zurück, deren Label exakt mit label übereinstimmt.\n","\n","\n","    \"\"\"\n","\n","    base = \"https://www.wikidata.org/w/api.php\"\n","    params = {\n","        \"action\": \"wbsearchentities\",\n","        \"search\": label,\n","        \"language\": lang,\n","        \"format\": \"json\",\n","        \"type\": \"item\",\n","        \"limit\": 10,\n","        \"origin\": \"*\"\n","    }\n","    url = f\"{base}?{urllib.parse.urlencode(params)}\"\n","\n","    for attempt in range(1, MAX_RETRY + 1):\n","        try:\n","            hits = requests.get(url, timeout=30).json().get(\"search\", [])\n","            if hits:\n","                canonical = label.strip().lower()\n","                for h in hits:\n","                    if h.get(\"label\", \"\").strip().lower() == canonical:\n","                        return h[\"id\"]\n","                return None\n","        except Exception as e:\n","            if attempt == MAX_RETRY:\n","                print(f\"{label}: {e}\")\n","            time.sleep(1.5 * attempt)\n","    return None\n","\n","\n","data = json.loads(pathlib.Path(INPUT_FILE).read_text(encoding=\"utf-8\"))\n","\n","for sample in data:\n","\n","  for el in data[sample]:\n","\n","    s_label = el[0]\n","    o_label = el[1]\n","\n","    el.append({\"subject_id\":wikidata_id(s_label) or \"N/A\"})\n","    el.append({\"object_id\":wikidata_id(o_label) or \"N/A\"})\n","\n","    time.sleep(5)\n","\n","\n","pathlib.Path(OUTPUT_FILE).write_text(\n","    json.dumps(data, ensure_ascii=False, indent=2), \"utf-8\"\n",")\n","print(f\" Datei mit IDs geschrieben: {OUTPUT_FILE}\")"],"metadata":{"id":"5uvNBZn9IpGs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multilingualer Übersetzung der Few-Shots Beispiele mit Wikidata (factual data)"],"metadata":{"id":"38tShNRlWp5T"}},{"cell_type":"code","source":["\"\"\"\n","Wikidata‑Label‑Exporter – Multilinguale Version\n","\n","Dieses Skript liest ein Few‑Shot‑JSON im Format der Masterarbeit ein und\n","schreibt pro Zielsprache eine neue Datei, in der Subjekt‑ und Objekt‑Labels\n","nach den folgenden Regeln erscheinen:\n","\n","1. FULL_TRANSLATE  →  immer Subjekt und Objekt in allen 7 Sprachen\n","2. PARTIAL_TRANSLATE\n","   • Für Hindi (hi) & Thai (th):  immer beide Labels übersetzen\n","   • Für de, fr, it, pt, es:      Regeln laut PARTIAL_TRANSLATE‑Dict\n","       – True    → immer übersetzen\n","       – False  → immer englisches Fallback\n","3. NO_TRANSLATE    →  nur für hi & th übersetzen; sonst Englisch behalten\n","\n","Falls kein Q‑ID‑Eintrag existiert oder Wikidata kein Label liefert, wird ein\n","Leerstring (\"\") ausgegeben.\n","\n","\"\"\"\n","\n","from __future__ import annotations\n","\n","import json\n","import requests\n","from pathlib import Path\n","from time import sleep\n","from typing import Dict, List, Tuple, Set\n","\n","\n","INPUT_FILE = Path(\n","    \"/content/drive/MyDrive/master_thesis/data/fewshot_examples/factual/few_shots/wikidata_id_factual_en.json\"\n",")\n","OUTPUT_TEMPLATE = Path(\n","    \"/content/drive/MyDrive/master_thesis/data/fewshot_examples/factual/wikidata_translation\"\n",")\n","LANGS: List[str] = [\"de\", \"fr\", \"it\", \"pt\", \"hi\", \"es\", \"th\"]\n","\n","MAX_TIMEOUT = 15\n","API_PAUSE_S = 3\n","\n","#  Übersetzungsregeln per Relation\n","FULL_TRANSLATE: List[str] = [\n","    \"city_in_country\",      # Stadt | Land\n","    \"country_capital_city\", # Land  | Hauptstadt\n","    \"country_currency\",     # Land  | Währung\n","    \"country_language\",     # Land  | Amtssprache\n","    \"country_largest_city\", # Land  | Größte Stadt\n","    \"food_from_country\",    # Gericht| Land\n","    \"landmark_in_country\",  # Landmarke| Land\n","    \"landmark_on_continent\",# Landmarke| Kontinent\n","]\n","\n","PARTIAL_TRANSLATE: Dict[str, Tuple[str | bool, str | bool]] = {\n","    \"company_hq\":                     (False, True),\n","    \"person_occupation\":              (False, True),\n","    \"person_plays_instrument\":        (False, True),\n","    \"person_plays_position_in_sport\": (False, True),\n","    \"person_plays_pro_sport\":         (False, True),\n","    \"person_university\":              (False, True),\n","    \"star_constellation\":             (False, True),\n","}\n","\n","NO_TRANSLATE: List[str] = [\n","    \"company_ceo\",\n","    \"person_band_lead_singer\",\n","    \"person_father\",\n","    \"person_mother\",\n","    \"pokemon_evolutions\",\n","    \"presidents_birth_year\",\n","    \"presidents_election_year\",\n","    \"product_by_company\",\n","    \"superhero_archnemesis\",\n","    \"superhero_person\",\n","]\n","\n","LATIN_LANGS: Set[str] = {\"de\", \"fr\", \"it\", \"pt\", \"es\"}\n","ASIAN_LANGS: Set[str] = {\"hi\", \"th\"}\n","\n","with INPUT_FILE.open(encoding=\"utf-8\") as fh:\n","    data_in: Dict[str, List[List]] = json.load(fh)\n","\n","all_qids: Set[str] = set()\n","for samples in data_in.values():\n","    for s in samples:\n","        all_qids.add(s[2].get(\"subject_id\", \"\"))\n","        all_qids.add(s[3].get(\"object_id\", \"\"))\n","\n","all_qids = {qid for qid in all_qids if qid.startswith(\"Q\")}\n","print(f\" Sammle Labels für {len(all_qids)} eindeutige Q‑IDs …\")\n","\n","#  Alle Labels von Wikidata holen (ein Call je Q‑ID)\n","ALL_LANGS = LANGS + [\"en\"]\n","label_cache: Dict[str, Dict[str, str]] = {\n","    qid: {lang: \"\" for lang in ALL_LANGS} for qid in all_qids\n","}\n","\n","for qid in all_qids:\n","    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n","    try:\n","        resp = requests.get(url, timeout=MAX_TIMEOUT)\n","        resp.raise_for_status()\n","        labels = resp.json()[\"entities\"][qid][\"labels\"]\n","        for lang in ALL_LANGS:\n","            if lang in labels:\n","                label_cache[qid][lang] = labels[lang][\"value\"]\n","    except Exception as exc:\n","        print(f\"  {qid}: {exc}\")\n","    sleep(API_PAUSE_S)\n","\n","print(\"Alle Wikidata‑Aufrufe abgeschlossen.\")\n","\n","#Entscheidungslogik, ob ein Label übersetzt werden soll\n","\n","def should_translate(rel: str, role: str, lang: str, qid: str) -> bool:\n","    \"\"\"Gibt True zurück, wenn das Lokalisieren des Labels für diese\n","    Relation/Position/Zielsprache erwünscht ist.\"\"\"\n","    #  FULL\n","    if rel in FULL_TRANSLATE:\n","        return True\n","\n","    # PARTIAL\n","    if rel in PARTIAL_TRANSLATE:\n","        subj_rule, obj_rule = PARTIAL_TRANSLATE[rel]\n","        rule = subj_rule if role == \"subject\" else obj_rule\n","\n","        # Hindi & Thai → immer lokalisieren\n","        if lang in ASIAN_LANGS:\n","            return True\n","\n","        # Lateinische Sprachen → Flag auswerten\n","        if rule is True:\n","            return True\n","        if rule is False:\n","            return False\n","        if rule == \"maybe\":\n","            # nur übersetzen, wenn Wikidata ein Label hat\n","            return bool(label_cache.get(qid, {}).get(lang))\n","        return False\n","\n","    # NO_TRANSLATE\n","    if rel in NO_TRANSLATE:\n","        return lang in ASIAN_LANGS  # only hi/th\n","\n","    return True\n","\n","\n","def get_label(qid: str, lang: str) -> str:\n","    \"\"\"Hilfsfunktion: Holt Label aus Cache oder gibt Leerstring zurück.\"\"\"\n","    return label_cache.get(qid, {}).get(lang, \"\")\n","\n","for lang in LANGS:\n","    data_out: Dict[str, List[List[str]]] = {}\n","\n","    for rel, samples in data_in.items():\n","        translated_samples: List[List[str]] = []\n","\n","        for s in samples:\n","            subj_q = s[2].get(\"subject_id\", \"\")\n","            obj_q = s[3].get(\"object_id\", \"\")\n","\n","            # Subjekt‑Label\n","            if subj_q:\n","                subj_lbl = (\n","                    get_label(subj_q, lang)\n","                    if should_translate(rel, \"subject\", lang, subj_q)\n","                    else get_label(subj_q, \"en\")\n","                )\n","            else:\n","                subj_lbl = \"\"\n","\n","            # Objekt‑Label\n","            if obj_q:\n","                obj_lbl = (\n","                    get_label(obj_q, lang)\n","                    if should_translate(rel, \"object\", lang, obj_q)\n","                    else get_label(obj_q, \"en\")\n","                )\n","            else:\n","                obj_lbl = \"\"\n","\n","            translated_samples.append([subj_lbl, obj_lbl])\n","\n","        data_out[rel] = translated_samples\n","\n","    out_path = OUTPUT_TEMPLATE.with_name(f\"{OUTPUT_TEMPLATE.name}_{lang}.json\")\n","    out_path.parent.mkdir(parents=True, exist_ok=True)\n","    with out_path.open(\"w\", encoding=\"utf-8\") as fh:\n","        json.dump(data_out, fh, ensure_ascii=False, indent=2)\n","\n","    print(f\" {lang.upper()}‑Datei geschrieben → {out_path}\")\n","\n","print(\"\\n Fertig!  Alle sieben Sprachdateien liegen im Zielordner.\")\n"],"metadata":{"id":"slCjhYvdVVEe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multilingualer Übersetzung der Samples mit Wikidata (factual data)"],"metadata":{"id":"6LyZWCBeWp8o"}},{"cell_type":"code","source":["\n","\"\"\"\"\n","FULL_TRANSLATE = [\n","    \"city_in_country\",\n","    \"country_capital_city\",\n","    \"country_currency\",\n","    \"country_language\",\n","    \"country_largest_city\",\n","]\n","\"\"\"\n","import json\n","import requests\n","from time import sleep\n","\n","# Eingabedatei und Zielsprachen\n","INPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata/country_currency_with_id.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata_translation/country_currency_wikidata_translated.json\"\n","\n","import json\n","import pathlib\n","import requests\n","from time import sleep\n","\n","\n","TARGET_LANGS = {\n","    \"de\": \"German\",\n","    \"fr\": \"French\",\n","    \"it\": \"Italian\",\n","    \"pt\": \"Portuguese\",\n","    \"hi\": \"Hindi\",\n","    \"es\": \"Spanish\",\n","    \"th\": \"Thai\",\n","}\n","\n","API_PAUSE_S = 3\n","MAX_TIMEOUT = 15\n","\n","_label_cache: dict[str, dict[str, str]] = {}\n","\n","def get_wikidata_full_translation(qid: str, langs: list[str]) -> dict[str, str]:\n","    \"\"\"\n","    Liefert ein Dict {lang: label} für die gewünschten Sprachen.\n","    Ergebnisse werden in einem einfachen In-Memory-Cache gehalten,\n","    um Mehrfachaufrufe für dieselbe Q-ID zu sparen.\n","    \"\"\"\n","    if qid in _label_cache:                 # Cache-Hit\n","        return {lang: _label_cache[qid].get(lang, \"\")\n","                for lang in langs}\n","\n","    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n","\n","    try:\n","        resp = requests.get(url, timeout=MAX_TIMEOUT)\n","        resp.raise_for_status()\n","        data   = resp.json()\n","        labels = data[\"entities\"][qid][\"labels\"]\n","        result = {lang: labels[lang][\"value\"] for lang in langs if lang in labels}\n","        _label_cache[qid] = result\n","        return result\n","    except Exception as e:\n","        print(f\"Error fetching {qid}: {e}\")\n","        return {}\n","\n","with open(INPUT_FILE, encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","for sample in data[\"samples\"]:\n","    for role in (\"subject\", \"object\"):\n","        qid = sample.get(f\"{role}_id\")\n","\n","        if qid and qid.startswith(\"Q\"):\n","            translations = get_wikidata_full_translation(qid, list(TARGET_LANGS.keys()))\n","        else:\n","            translations = {lang: \"\" for lang in TARGET_LANGS}\n","\n","        for lang in TARGET_LANGS:\n","            sample[f\"{role}_{lang}\"] = translations.get(lang, \"\")\n","\n","    sleep(API_PAUSE_S)\n","\n","with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","try:\n","    import pandas as pd\n","    df = pd.DataFrame(data[\"samples\"])\n","    print(df.head())\n","except ImportError:\n","    pass\n","\n","\n"],"metadata":{"id":"bpNDNvMq03RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import requests\n","from time import sleep\n","\n","\n","\n","\n","INPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata/person_university_with_id.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata_translation/person_university_wikidata_translated.json\"\n","\n","TARGET_LANGS = {\n","    \"de\": \"German\",\n","    \"fr\": \"French\",\n","    \"it\": \"Italian\",\n","    \"pt\": \"Portuguese\",\n","    \"hi\": \"Hindi\",\n","    \"es\": \"Spanish\",\n","    \"th\": \"Thai\",\n","}\n","\n","API_PAUSE_S = 3\n","MAX_TIMEOUT = 15\n","\n","_label_cache: dict[str, dict[str, str]] = {}\n","\n","def get_wikidata_partial_translation(qid: str, langs: list[str]) -> dict[str, str]:\n","    if qid in _label_cache:\n","        return {lang: _label_cache[qid].get(lang, \"\") for lang in langs}\n","\n","    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n","    try:\n","        resp = requests.get(url, timeout=MAX_TIMEOUT)\n","        resp.raise_for_status()\n","        data = resp.json()\n","        labels = data[\"entities\"][qid][\"labels\"]\n","        result = {lang: labels[lang][\"value\"] for lang in langs if lang in labels}\n","        _label_cache[qid] = result\n","        return result\n","    except Exception as e:\n","        print(f\"Error fetching {qid}: {e}\")\n","        return {}\n","\n","# JSON laden\n","with open(INPUT_FILE, encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","# Samples verarbeiten\n","for sample in data[\"samples\"]:\n","    for role in (\"subject\", \"object\"):\n","        qid = sample.get(f\"{role}_id\")\n","\n","        # Objekt wird in alle Sprachen übersetzt\n","        if role == \"object\":\n","            translations = get_wikidata_partial_translation(qid, list(TARGET_LANGS.keys())) if qid and qid.startswith(\"Q\") else {lang: \"\" for lang in TARGET_LANGS}\n","            for lang in TARGET_LANGS:\n","                sample[f\"{role}_{lang}\"] = translations.get(lang, \"\")\n","\n","        # Subjekt: nur hi und th übersetzen, sonst Englisch übernehmen\n","        elif role == \"subject\":\n","            translations = get_wikidata_partial_translation(qid, [\"hi\", \"th\"]) if qid and qid.startswith(\"Q\") else {lang: \"\" for lang in [\"hi\", \"th\"]}\n","            for lang in TARGET_LANGS:\n","                if lang in {\"hi\", \"th\"}:\n","                    sample[f\"{role}_{lang}\"] = translations.get(lang, \"\")\n","                else:\n","                    sample[f\"{role}_{lang}\"] = sample[\"subject\"]  # Englisch übernehmen\n","\n","    sleep(API_PAUSE_S)\n","\n","# JSON speichern\n","with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","# Vorschau\n","try:\n","    import pandas as pd\n","    df = pd.DataFrame(data[\"samples\"])\n","    print(df.head())\n","except ImportError:\n","    pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axaRxWekdIbp","executionInfo":{"status":"ok","timestamp":1750196030064,"user_tz":-120,"elapsed":310782,"user":{"displayName":"Aldi Halili","userId":"04451056983466283749"}},"outputId":"7e7c8b70-bd35-49d4-a817-bfe7e64c5052"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           subject                      object subject_id object_id  \\\n","0   Michelle Obama        Princeton University     Q13133    Q21578   \n","1       Bill Gates          Harvard University      Q5284    Q13371   \n","2  Mark Zuckerberg          Harvard University     Q36215    Q13371   \n","3    Oprah Winfrey  Tennessee State University     Q55800  Q1782948   \n","4      Emma Watson            Brown University     Q39476    Q49114   \n","\n","        subject_de       subject_fr       subject_it       subject_pt  \\\n","0   Michelle Obama   Michelle Obama   Michelle Obama   Michelle Obama   \n","1       Bill Gates       Bill Gates       Bill Gates       Bill Gates   \n","2  Mark Zuckerberg  Mark Zuckerberg  Mark Zuckerberg  Mark Zuckerberg   \n","3    Oprah Winfrey    Oprah Winfrey    Oprah Winfrey    Oprah Winfrey   \n","4      Emma Watson      Emma Watson      Emma Watson      Emma Watson   \n","\n","         subject_hi       subject_es            subject_th  \\\n","0       मिशेल ओबामा   Michelle Obama        มิเชลล์ โอบามา   \n","1         बिल गेट्स       Bill Gates             บิล เกตส์   \n","2  मार्क ज़ुकेरबर्ग  Mark Zuckerberg  มาร์ก ซักเคอร์เบิร์ก   \n","3      ओपरा विनफ्रे    Oprah Winfrey      โอปราห์ วินฟรีย์   \n","4         एमा वॉटसन      Emma Watson         เอ็มมา วอตสัน   \n","\n","                    object_de                       object_fr  \\\n","0        Princeton University         université de Princeton   \n","1          Harvard University              université Harvard   \n","2          Harvard University              université Harvard   \n","3  Tennessee State University  université d'État du Tennessee   \n","4            Brown University                université Brown   \n","\n","                    object_it                   object_pt  \\\n","0     Università di Princeton   Universidade de Princeton   \n","1       Università di Harvard        Universidade Harvard   \n","2       Università di Harvard        Universidade Harvard   \n","3  Tennessee State University  Tennessee State University   \n","4            Università Brown          Universidade Brown   \n","\n","                object_hi                 object_es             object_th  \n","0  प्रिंसटन विश्वविद्यालय  Universidad de Princeton  มหาวิทยาลัยพรินซ์ตัน  \n","1  हार्वर्ड विश्वविद्यालय       Universidad Harvard  มหาวิทยาลัยฮาร์วาร์ด  \n","2  हार्वर्ड विश्वविद्यालय       Universidad Harvard  มหาวิทยาลัยฮาร์วาร์ด  \n","3                                                                          \n","4                                 Universidad Brown     มหาวิทยาลัยบราวน์  \n"]}]},{"cell_type":"code","source":["import json\n","import requests\n","from time import sleep\n","\n","# Eingabedatei und Zieldatei\n","INPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata/pokemon_evolutions_with_id.json\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/master_thesis/data/factual_data/wikidata_translation/pokemon_evolutions_wikidata_translated.json\"\n","\n","TARGET_LANGS = {\n","    \"de\": \"German\",\n","    \"fr\": \"French\",\n","    \"it\": \"Italian\",\n","    \"pt\": \"Portuguese\",\n","    \"hi\": \"Hindi\",\n","    \"es\": \"Spanish\",\n","    \"th\": \"Thai\",\n","}\n","\n","API_PAUSE_S = 3\n","MAX_TIMEOUT = 15\n","\n","_label_cache: dict[str, dict[str, str]] = {}\n","\n","def get_wikidata_thai_hindi_translation(qid: str, langs: list[str]) -> dict[str, str]:\n","    if qid in _label_cache:\n","        return {lang: _label_cache[qid].get(lang, \"\") for lang in langs}\n","\n","    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n","    try:\n","        resp = requests.get(url, timeout=MAX_TIMEOUT)\n","        resp.raise_for_status()\n","        data = resp.json()\n","        labels = data[\"entities\"][qid][\"labels\"]\n","        result = {lang: labels[lang][\"value\"] for lang in langs if lang in labels}\n","        _label_cache[qid] = result\n","        return result\n","    except Exception as e:\n","        print(f\"Error fetching {qid}: {e}\")\n","        return {}\n","\n","# JSON laden\n","with open(INPUT_FILE, encoding=\"utf-8\") as f:\n","    data = json.load(f)\n","\n","# Samples durchlaufen & Sprachen setzen\n","for sample in data[\"samples\"]:\n","    for role in (\"subject\", \"object\"):\n","        qid = sample.get(f\"{role}_id\")\n","\n","        # Nur für hi und th übersetzen\n","        langs_to_translate = [\"hi\", \"th\"]\n","        translations = get_wikidata_thai_hindi_translation(qid, langs_to_translate) if qid and qid.startswith(\"Q\") else {lang: \"\" for lang in langs_to_translate}\n","\n","        for lang in TARGET_LANGS:\n","            if lang in langs_to_translate:\n","                sample[f\"{role}_{lang}\"] = translations.get(lang, \"\")\n","            else:\n","                sample[f\"{role}_{lang}\"] = sample[role]  # englischer Originalwert\n","\n","    sleep(API_PAUSE_S)\n","\n","# Ergebnis speichern\n","with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","# Vorschau\n","try:\n","    import pandas as pd\n","    df = pd.DataFrame(data[\"samples\"])\n","    print(df.head())\n","except ImportError:\n","    pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfzHB1ySwqi2","executionInfo":{"status":"ok","timestamp":1750202489100,"user_tz":-120,"elapsed":143421,"user":{"displayName":"Aldi Halili","userId":"04451056983466283749"}},"outputId":"c13ef592-aca6-4e2b-b05a-39c91545c798"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      subject      object subject_id object_id  subject_de  subject_fr  \\\n","0   Bulbasaur     Ivysaur    Q847571  Q1636903   Bulbasaur   Bulbasaur   \n","1  Charmander  Charmeleon   Q3178753  Q1637365  Charmander  Charmander   \n","2    Squirtle   Wartortle    Q845294  Q1752151    Squirtle    Squirtle   \n","3     Pikachu      Raichu      Q9351  Q1647331     Pikachu     Pikachu   \n","4      Oddish       Gloom   Q2002874  Q5571265      Oddish      Oddish   \n","\n","   subject_it  subject_pt subject_hi  subject_es   subject_th   object_de  \\\n","0   Bulbasaur   Bulbasaur              Bulbasaur  ฟุชิงิดาเนะ     Ivysaur   \n","1  Charmander  Charmander             Charmander   ฮิโตะคาเงะ  Charmeleon   \n","2    Squirtle    Squirtle               Squirtle    เซนิกาเมะ   Wartortle   \n","3     Pikachu     Pikachu     पिकाचु     Pikachu       พิคาชู      Raichu   \n","4      Oddish      Oddish                 Oddish                    Gloom   \n","\n","    object_fr   object_it   object_pt object_hi   object_es object_th  \n","0     Ivysaur     Ivysaur     Ivysaur               Ivysaur  ฟุชิกิโซ  \n","1  Charmeleon  Charmeleon  Charmeleon            Charmeleon   ลิซาโดะ  \n","2   Wartortle   Wartortle   Wartortle             Wartortle     คาเมล  \n","3      Raichu      Raichu      Raichu                Raichu      ไรชู  \n","4       Gloom       Gloom       Gloom                 Gloom            \n"]}]},{"cell_type":"markdown","source":["## testen der Entitäten oder IDs in Wikidata"],"metadata":{"id":"LvXb-oSIzZ8u"}},{"cell_type":"code","source":["#testen nur die Entitäten oder id\n","\n","import requests, urllib.parse, json\n","term=\"Microsoft\"\n","#Bengaluru\n","\n","url = (\"https://www.wikidata.org/w/api.php?\"\n","       \"action=wbsearchentities&format=json&language=en&type=item&limit=10&\"\n","       f\"search={urllib.parse.quote(term)}\")\n","hits = requests.get(url).json()[\"search\"]\n","for h in hits:\n","\n","  descr = h.get(\"description\", \"\").lower()\n","  #if (\"contry\" in descr) or (\"currency\" in descr):\n","  print(h[\"id\"], \"→\", h[\"label\"], \"|\", descr)\n","    #o_label = sample[\"object\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uQz0DkG4zzd","executionInfo":{"status":"ok","timestamp":1750337644985,"user_tz":-120,"elapsed":214,"user":{"displayName":"Aldi Halili","userId":"04451056983466283749"}},"outputId":"0633d810-40c6-4067-8e2d-1ae38b2a06c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Q2283 → Microsoft | american multinational technology corporation\n","Q1406 → Microsoft Windows | family of computer operating systems developed by microsoft\n","Q135288 → Microsoft Store | digital distribution platform from microsoft\n","Q132020 → Xbox | video game console by microsoft\n","Q11215 → Windows 7 | personal computer operating system by microsoft that was released in 2009\n","Q11272 → Microsoft Excel | spreadsheet editor, part of microsoft 365\n","Q11230 → Windows Vista | personal computer operating system by microsoft that was released in 2007\n","Q5046 → Windows 8 | personal computer operating system by microsoft that was released in 2012\n","Q60683589 → Microsoft Academic Knowledge Graph | rdf representation of the microsoft academic graph\n","Q83370 → Windows 95 | personal computer operating system by microsoft\n"]}]}]}